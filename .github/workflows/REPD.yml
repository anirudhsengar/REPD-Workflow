name: Bug Prediction

on:
  pull_request_target:
    branches:
      - main

permissions:
  pull-requests: write
  issues: write

jobs:
  predict:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install tensorflow==2.12.0 pandas joblib
        sudo apt-get update
        sudo apt-get install -y cloc

    - name: Get changed files
      id: changed-files
      run: |
        files=$(git diff --name-only --diff-filter=ACMRT ${{ github.event.pull_request.base.sha }}..${{ github.event.pull_request.head.sha }} | grep -E "\.(c|cpp|cxx|cc|h|hpp|hxx)\$" | xargs)
        echo "all_changed_files=$files" >> $GITHUB_OUTPUT
        if [ -n "$files" ]; then
          echo "any_changed=true" >> $GITHUB_OUTPUT
        else
          echo "any_changed=false" >> $GITHUB_OUTPUT
        fi

    - name: Extract features BEFORE changes (base)
      if: steps.changed-files.outputs.any_changed == 'true'
      run: |
        git checkout ${{ github.event.pull_request.base.sha }}
        chmod +x ./ExtractTraditionalFeatures.sh
        
        # Create base metrics directory
        mkdir -p metrics_output_base
        
        # Initialize combined CSV with header
        echo "File,loc,v(g),ev(g),iv(g),n,v,l,d,i,e,b,t,lOComment,lOBlank,LOCodeAndComment,uniq_Op,Uniq_Opnd,total_Op,total_Opnd,branchCount" > metrics_output_base/summary_metrics.csv
        
        # Process each file and accumulate results
        for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
          echo "Processing $file for base metrics..."
          ./ExtractTraditionalFeatures.sh "$file"
          
          # Find the generated directory and append its CSV data (skip header)
          generated_dir=$(ls -td metrics_output_* | head -n 1)
          if [ -d "$generated_dir" ] && [ -f "$generated_dir/summary_metrics.csv" ]; then
            # Append data rows (skip header line)
            tail -n +2 "$generated_dir/summary_metrics.csv" >> metrics_output_base/summary_metrics.csv
            # Clean up the temporary directory
            rm -rf "$generated_dir"
          fi
        done

    - name: Extract features AFTER changes (head)
      if: steps.changed-files.outputs.any_changed == 'true'
      run: |
        git checkout ${{ github.event.pull_request.head.sha }}
        chmod +x ./ExtractTraditionalFeatures.sh
        
        # Create head metrics directory
        mkdir -p metrics_output_head
        
        # Initialize combined CSV with header
        echo "File,loc,v(g),ev(g),iv(g),n,v,l,d,i,e,b,t,lOComment,lOBlank,LOCodeAndComment,uniq_Op,Uniq_Opnd,total_Op,total_Opnd,branchCount" > metrics_output_head/summary_metrics.csv
        
        # Process each file and accumulate results
        for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
          echo "Processing $file for head metrics..."
          ./ExtractTraditionalFeatures.sh "$file"
          
          # Find the generated directory and append its CSV data (skip header)
          generated_dir=$(ls -td metrics_output_* | head -n 1)
          if [ -d "$generated_dir" ] && [ -f "$generated_dir/summary_metrics.csv" ]; then
            # Append data rows (skip header line)
            tail -n +2 "$generated_dir/summary_metrics.csv" >> metrics_output_head/summary_metrics.csv
            # Clean up the temporary directory
            rm -rf "$generated_dir"
          fi
        done

    - name: Debug accumulated metrics
      if: steps.changed-files.outputs.any_changed == 'true'
      run: |
        echo "=== Changed files ==="
        echo "${{ steps.changed-files.outputs.all_changed_files }}"
        echo "=== Base metrics CSV ==="
        cat metrics_output_base/summary_metrics.csv
        echo "=== Head metrics CSV ==="
        cat metrics_output_head/summary_metrics.csv
        echo "=== Row counts ==="
        echo "Base rows: $(wc -l < metrics_output_base/summary_metrics.csv)"
        echo "Head rows: $(wc -l < metrics_output_head/summary_metrics.csv)"

    - name: Verify or create trained model
      if: steps.changed-files.outputs.any_changed == 'true'
      run: |
        if [ ! -d "trained_model" ] || [ ! -f "trained_model/metadata.json" ]; then
          echo "Training new model..."
          python save_trained_model.py
        else
          echo "Trained model found âœ…"
        fi

    - name: Run prediction
      if: steps.changed-files.outputs.any_changed == 'true'
      id: prediction
      run: |
        echo "Running predictions with trained model..."
        output_base=$(python predict.py metrics_output_base/summary_metrics.csv)
        output_head=$(python predict.py metrics_output_head/summary_metrics.csv)
        {
          echo "comment<<EOF"
          echo "## ðŸ“Š Bug Risk Analysis"
          echo ""
          echo "### ðŸ”„ BEFORE PR Changes:"
          echo "$output_base"
          echo ""
          echo "### âœ… AFTER PR Changes:"
          echo "$output_head"
          echo ""
          echo "Interpretation Note:"
          echo "> The values shown are Probability Densities (PDFs), not probabilities. They represent the model's assessment of how likely a file's characteristics are to be 'defective' vs. 'non-defective'. A higher value indicates a better fit for that category. Very small values are expected and normal."
          echo ""
          echo "EOF"
        } >> $GITHUB_OUTPUT

    - name: Comment on PR
      if: steps.changed-files.outputs.any_changed == 'true'
      uses: actions/github-script@v6
      env:
        COMMENT_BODY: "${{ steps.prediction.outputs.comment }}"
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          github.rest.issues.createComment({
            issue_number: context.payload.pull_request.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: process.env.COMMENT_BODY
          })